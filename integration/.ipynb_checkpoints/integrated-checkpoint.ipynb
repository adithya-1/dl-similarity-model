{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import scipy.io.wavfile as wavf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelOne = load_model(\"../wordPredictor/modelOne/modelOne.h5\")\n",
    "modelTwo = load_model(\"../wordPredictor/modelTwo/modelTwo.h5\")\n",
    "modelThree = load_model(\"../wordPredictor/modelThree/modelThree.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../wordPredictor/modelOne/tokenizerOne.tokenizer', 'rb') as config_dictionary_file:\n",
    "    tokenizerOne = pickle.load(config_dictionary_file)\n",
    "\n",
    "with open('../wordPredictor/modelTwo/tokenizerTwo.tokenizer', 'rb') as config_dictionary_file:\n",
    "    tokenizerTwo = pickle.load(config_dictionary_file)\n",
    "    \n",
    "with open('../wordPredictor/modelThree/tokenizerThree.tokenizer', 'rb') as config_dictionary_file:\n",
    "    tokenizerThree = pickle.load(config_dictionary_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_1 = 'nextTurn.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_wavfile(filename):\n",
    "    \n",
    "    def isNaN(x):\n",
    "        return x != x\n",
    "    signal,sample_rate = librosa.load(filename)\n",
    "    \n",
    "    for i in range(len(signal)):\n",
    "        if signal[i] < 0:\n",
    "            signal[i] = -signal[i]\n",
    "    \n",
    "    index_list = []\n",
    "    j = 0\n",
    "    while j < len(signal):\n",
    "        index_list.append(j)\n",
    "        j = j + 1\n",
    "        \n",
    "    df = pd.DataFrame(list(zip(index_list, signal)),columns =['time', 'signal'])\n",
    "    \n",
    "    df['mov_avg'] = df['signal'].rolling(10000).sum()\n",
    "    \n",
    "    smooth_list = df['mov_avg'].tolist()\n",
    "    #print(len(smooth_list))\n",
    "    \n",
    "    minimas = []\n",
    "    i = 1\n",
    "    while isNaN(smooth_list[i]):\n",
    "        i = i + 1\n",
    "    \n",
    "    #print(i)\n",
    "    window = 10000\n",
    "\n",
    "    while i + window < len(smooth_list):\n",
    "        #print(\"hi\")\n",
    "        m = min(smooth_list[i:i+window])\n",
    "        idx = smooth_list.index(m)\n",
    "        #print(m,idx,smooth_list[i],smooth_list[i+window-1])\n",
    "        \n",
    "        if m != smooth_list[i] and m != smooth_list[i+window-1]:\n",
    "            if len(minimas) != 0  and abs(idx - minimas[-1][0]) > 5000:  \n",
    "                minimas.append([idx,m])\n",
    "            if len(minimas) == 0:\n",
    "                minimas.append([idx,m])\n",
    "    \n",
    "        i = i + window\n",
    "\n",
    "    print(minimas)\n",
    "    return minimas\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35582, 215.87235487032842], [54520, 59.05934052937968], [70084, 410.27646079503484]]\n"
     ]
    }
   ],
   "source": [
    "split_indexes = split_wavfile(filename_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35582, 215.87235487032842], [54520, 59.05934052937968], [70084, 410.27646079503484]]\n"
     ]
    }
   ],
   "source": [
    "print(split_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordPredictor(sentence):\n",
    "    sol = []\n",
    "    \n",
    "    if len(sentence.split()) == 1:\n",
    "        encoded_text = tokenizerOne.texts_to_sequences([sentence])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=1, truncating='pre')\n",
    "        print(encoded_text, pad_encoded)\n",
    "        for i in (modelOne.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
    "          pred_word = tokenizerOne.index_word[i]\n",
    "          sol.append(pred_word)\n",
    "            \n",
    "    elif len(sentence.split()) == 2:\n",
    "        encoded_text = tokenizerTwo.texts_to_sequences([sentence])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=2, truncating='pre')\n",
    "        print(encoded_text, pad_encoded)\n",
    "        for i in (modelTwo.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
    "          pred_word = tokenizerTwo.index_word[i]\n",
    "          sol.append(pred_word)\n",
    "            \n",
    "    else:\n",
    "        encoded_text = tokenizerThree.texts_to_sequences([sentence])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=3, truncating='pre')\n",
    "        print(encoded_text, pad_encoded)\n",
    "        for i in (modelThree.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
    "          pred_word = tokenizerThree.index_word[i]\n",
    "          sol.append(pred_word)\n",
    "            \n",
    "    return sol\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_send(filename,split_indexes):\n",
    "    \n",
    "    SIGNAL,sample_rate = librosa.load(filename)\n",
    "    first_signal = SIGNAL[:split_indexes[0][0]]\n",
    "    cur_sentence = \"s\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    classes = wordPredictor(cur_sentence)\n",
    "\n",
    "    #classes = [\"plumbing\",\"iro\",\"service\",\"when\"]\n",
    "    print(\"top three = \", classes)\n",
    "    cur_sentence = cur_sentence + \" \" + predict(first_signal,classes)\n",
    "    \n",
    "    \n",
    "    for i in range(1,len(split_indexes)):\n",
    "        X = SIGNAL[split_indexes[i-1][0]:split_indexes[i][0]]\n",
    "        classes = wordPredictor(cur_sentence)\n",
    "        print(\"top three = \", classes)\n",
    "        cur_sentence = cur_sentence + \" \" + predict(X,classes)\n",
    "        \n",
    "    X = SIGNAL[split_indexes[-1][0]:]\n",
    "    classes = wordPredictor(cur_sentence)\n",
    "    print(\"top three = \", classes)\n",
    "    cur_sentence = cur_sentence + \" \" + predict(X,classes)\n",
    "    \n",
    "    return cur_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(chunk_data,classes):\n",
    "    \n",
    "    df = pd.read_csv(r\"references.csv\")\n",
    "    \n",
    "    print(\"classes = \", classes)\n",
    "    \n",
    "    def features_extractor(file):\n",
    "        audio,sample_rate = librosa.load(file , res_type = 'kaiser_fast')\n",
    "        mfccs_features = librosa.feature.mfcc(y = audio,sr = sample_rate, n_mfcc=40)\n",
    "        mfccs_scaled_features = np.mean(mfccs_features.T,axis = 0)\n",
    "        return mfccs_scaled_features\n",
    "    \n",
    "    extracted_features = []\n",
    "    #classes = [\"plumbing\",\"iro\",\"service\",\"when\"]\n",
    "    #audio_path_dir =str(\"C:Users\\sai\\Desktop\\Capstone project\\DL\\coding\\audio_data\")\n",
    "    cwd = os.getcwd()\n",
    "    for i in df.index:\n",
    "        class_name = df['class_name'][i]\n",
    "\n",
    "        audio_file_name = str(df['file_name'][i])\n",
    "\n",
    "        if class_name in classes:\n",
    "            print(\"hi\")\n",
    "            #audio_path = cwd + \"\\\\audio_data\\\\\" + audio_file_name + \".wav\"\n",
    "            audio_path = \"D:\\PES\\Capstone\\phase two\" + \"\\wav\\\\\" +   audio_file_name\n",
    "            #print(audio_path)\n",
    "            data = features_extractor(audio_path)\n",
    "            extracted_features.append([class_name,data])\n",
    "            \n",
    "    df_extracted_features = pd.DataFrame(extracted_features,columns = ['class_name' , 'feature_vector'])\n",
    "    \n",
    "    mini_df = df_extracted_features\n",
    "    \n",
    "    X = np.array(mini_df['feature_vector'].tolist())\n",
    "    y = np.array(mini_df['class_name'].tolist())\n",
    "    \n",
    "    #y = np.array(pd.get_dummies(y))\n",
    "    labelencoder=LabelEncoder()\n",
    "    y=to_categorical(labelencoder.fit_transform(y))\n",
    "    \n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state = 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    num_labels=y.shape[1]\n",
    "    model=Sequential()\n",
    "    ###first layer\n",
    "    model.add(Dense(100,input_shape=(40,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    ###second layer\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    ###third layer\n",
    "    model.add(Dense(100))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    ###final layer\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "    from datetime import datetime \n",
    "\n",
    "    num_epochs = 250\n",
    "    num_batch_size = 4\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), verbose=0)\n",
    "    test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "    print(test_accuracy[1])\n",
    "    \n",
    "    \n",
    "    #Preditcting the class\n",
    "    \n",
    "    mfccs_features = librosa.feature.mfcc(y = chunk_data , sr=22050, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "    \n",
    "    predicted_label = model.predict_classes(mfccs_scaled_features)\n",
    "    \n",
    "    prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "    print(prediction_class[0])\n",
    "    return prediction_class[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-45100899d45d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-dde35c4f9327>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(chunk_data, classes)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m#y = np.array(pd.get_dummies(y))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mlabelencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     73\u001b[0m   \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m   \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2666\u001b[0m     \"\"\"\n\u001b[0;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[1;32m-> 2668\u001b[1;33m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "signal,sample_rate = librosa.load(filename_1)\n",
    "predict(signal,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [[1]]\n",
      "top three =  ['what', 'hathira', 'hathiradha']\n",
      "classes =  ['what', 'hathira', 'hathiradha']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-445b2251e61e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msplit_and_send\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_indexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-bf14014758a6>\u001b[0m in \u001b[0;36msplit_and_send\u001b[1;34m(filename, split_indexes)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#classes = [\"plumbing\",\"iro\",\"service\",\"when\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"top three = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mcur_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_sentence\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_signal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-0985d3310a9c>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(chunk_data, classes)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#y = np.array(pd.get_dummies(y))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mlabelencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     73\u001b[0m   \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m   \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2666\u001b[0m     \"\"\"\n\u001b[0;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[1;32m-> 2668\u001b[1;33m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "split_and_send(filename_1,split_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text(filename):\n",
    "    \n",
    "    split_indexes = split_wavfile(filename)\n",
    "    predicted_sentence = split_and_send(filename,split_indexes)\n",
    "    print(predicted_sentence)\n",
    "    return predicted_sentence\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_1 = 'nannaNext.wav'\n",
    "final_output = speech_to_text(filename_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
