{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.utils import to_categorical\n",
    "from doc3 import training_doc3\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = re.sub(r'\\W+', ' ', training_doc3).lower()\n",
    "tokens = word_tokenize(cleaned)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 3\n",
    "text_sequences = []\n",
    "for i in range(train_len,len(tokens)+1):\n",
    "  seq = tokens[i-train_len:i]\n",
    "  text_sequences.append(seq)\n",
    "print(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequence_1 = []\n",
    "for i in text_sequences:\n",
    "    if \"s\" in i:\n",
    "        if i[0] != \"s\":\n",
    "            continue\n",
    "    if \"e\" in i:\n",
    "        if i[3] != \"e\":\n",
    "            continue\n",
    "    \n",
    "    text_sequence_1.append(i)\n",
    "print(text_sequence_1)\n",
    "text_sequences = copy.deepcopy(text_sequence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {}\n",
    "count = 1\n",
    "for i in range(len(tokens)):\n",
    "  if tokens[i] not in sequences:\n",
    "    sequences[tokens[i]] = count\n",
    "    count += 1\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "print(len(text_sequences))\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)+1\n",
    "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "print(n_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sequences)):\n",
    "  n_sequences[i] = sequences[i]\n",
    "print(n_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = n_sequences[:,:-1]\n",
    "train_targets = n_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\n",
    "seq_len = train_inputs.shape[1]\n",
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "# compiling the network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_inputs,train_targets,epochs=500,verbose=1)\n",
    "model.save(\"mymodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "input_text = input().strip().lower()\n",
    "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "print(encoded_text, pad_encoded)\n",
    "for i in (model.predict(pad_encoded)[0]).argsort()[-10:][::-1]:\n",
    "  pred_word = tokenizer.index_word[i]\n",
    "  print(\"Next word suggestion:\",pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "#config_dictionary = {'remote_hostname': 'google.com', 'remote_port': 80}\n",
    " \n",
    "# Step 2\n",
    "with open('config.dictionary', 'wb') as config_dictionary_file:\n",
    " \n",
    "  # Step 3\n",
    "  pickle.dump(tokenizer, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
